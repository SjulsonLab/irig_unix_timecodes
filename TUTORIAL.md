# Tutorial: Decoding IRIG Timecodes with NeuroKairos

## The Problem: Multiple Clocks

Multi-modal neuroscience experiments produce data from independent systems — electrophysiology rigs, video cameras, behavior controllers — each running on its own clock. These clocks drift relative to each other (typically 10–100 ppm), so you can't simply line up timestamps by assuming they started at the same time. To combine these streams in pynapple, you need a common time reference.

IRIG timecodes solve this by embedding a UTC (Coordinated Universal Time) time signal directly into each recording. A dedicated device (the **encoder**) generates the IRIG signal, which gets recorded alongside neural data on a spare analog channel or displayed as a blinking LED visible to cameras. By running the **decoder** on each recording independently, you anchor every stream to the same UTC reference — clock drift and all. The resulting `ClockTable` acts as the **synchronizer**, bridging each device's local clock to UTC.

## What is IRIG?

IRIG-H is a pulse-width-modulated timecode: one pulse per second, each encoding a binary digit. Pulse widths encode bit values: 0.2 seconds = binary 0, 0.5 seconds = binary 1, 0.8 seconds = reference marker. A complete frame is 60 pulses (one minute), encoding the current UTC time (minutes, hours, day-of-year, year) in Binary Coded Decimal (BCD).

Within each frame, marker pulses at fixed positions act as delimiters. Frame boundaries are identified by two consecutive marker pulses — the last pulse of one frame and the first of the next.

In a typical setup, a Raspberry Pi running [NeuroKairos](https://github.com/evilrobotxoxo/irig_unix_timecodes) acts as the encoder, generating the IRIG signal that gets split and fed to each recording system. The decoder in `neurokairos` handles real-world issues automatically: noise spikes (extra pulses), signal dropouts (missing pulses), and concatenated files where multiple recording segments are stitched together.

## The Core Abstraction: ClockTable

Every decoder in `neurokairos` produces a `ClockTable` — a sparse mapping between two time domains:

- **source**: values in the recording's native domain (sample indices for electrophysiology, frame indices for video, seconds for pre-extracted intervals)
- **reference**: UTC time in Unix format (the number of seconds since Jan. 1, 1970) when the corresponding source value was collected

There's roughly one entry per IRIG pulse (~1 per second). Between entries, `np.interp` handles the conversion. The ClockTable is the synchronizer — it captures the non-linear relationship between the local clock and UTC, correcting for drift.

```python
import neurokairos

ct = neurokairos.decode_dat_irig("recording.dat", n_channels=3, irig_channel=2)
print(ct)
# ClockTable: 238 entries (samples), rate=30000.0
#   recording: 2025-01-15T14:30:37Z → 2025-01-15T14:34:35Z
#   file: recording.dat (/data/ephys/recording.dat)
#   source=[6000.0..7134000.0], reference=[1736950237.0..1736950475.0]

# Convert sample indices to UTC timestamps
import numpy as np
timestamps = ct.source_to_reference(np.array([30000, 60000, 90000]))

# Convert UTC timestamps back to sample indices
samples = ct.reference_to_source(np.array([1736950240.0, 1736950250.0]))
```

### Metadata and Decoding Quality

Every ClockTable carries a `metadata` dict with provenance, decoding quality, and NTP sync status. Decoders populate this automatically.

```python
ct = neurokairos.decode_dat_irig("recording.dat", n_channels=3, irig_channel=2)

# Provenance — where did this come from?
ct.metadata["source_file"]     # "recording.dat"
ct.metadata["source_path"]     # "/data/ephys/recording.dat"
ct.metadata["n_channels"]      # 3
ct.metadata["irig_channel"]    # 2

# Recording times (ISO 8601 UTC)
ct.metadata["recording_start"]  # "2025-01-15T14:30:37Z"
ct.metadata["recording_stop"]   # "2025-01-15T14:34:35Z"

# Decoding quality — how clean was the signal?
ct.metadata["n_raw_pulses"]     # total pulses detected
ct.metadata["n_extra_removed"]  # noise spikes removed
ct.metadata["n_missing_gaps"]   # gaps from missing pulses
ct.metadata["n_frames_decoded"] # complete IRIG frames decoded
ct.metadata["n_concat_boundaries"]  # concatenation boundaries found
```

**NTP sync status**: If the IRIG signal was generated by NeuroKairos, the decoder extracts chrony NTP sync quality from previously unused bits near the end of each IRIG frame. This tells you how well the encoder was synchronized to UTC:

```python
ct.metadata["stratum"]              # worst-case NTP stratum (1–4; 4 = not synchronized)
ct.metadata["UTC_sync_precision"]   # worst-case root dispersion, e.g. "< 0.25 ms"
```

Stratum 1–2 with sub-millisecond dispersion is typical for a well-configured NeuroKairos setup. Higher stratum or larger dispersion means the encoder's clock was less tightly locked to UTC during the recording. Since these bits are unused in standard IRIG-H, old recordings (without sync status encoding) have all-zero bits — this is ambiguous with genuinely excellent sync (stratum 1, < 0.25 ms).

Metadata persists through `save()`/`load()` — it's serialized as JSON inside the NPZ file.

### Decode Once, Reload Many Times

IRIG decoding is the slow step — it reads the entire recording to find and classify pulses. The resulting ClockTable is small (a few KB). Save it once, reload instantly:

```python
# First time: decode and auto-save (default behavior)
ct = neurokairos.decode_dat_irig("recording.dat", n_channels=3, irig_channel=2)
# → saves recording.dat.clocktable.npz

# Every subsequent time: reload in milliseconds
ct = neurokairos.ClockTable.load("recording.dat.clocktable.npz")
```

All `decode_*` functions auto-save by default (except `decode_intervals_irig`, which has no input file to derive a path from — pass `save="path.npz"` explicitly).

## IRIGDecoder: Unified API

`IRIGDecoder` provides a single facade over all decoder functions. Use one of the `from_*` classmethods to create an instance, then call `decode()`:

```python
from neurokairos import IRIGDecoder

# Each classmethod mirrors the corresponding decode_* function
decoder = IRIGDecoder.from_sglx("recording.bin", irig_channel="sync")
decoder = IRIGDecoder.from_dat("recording.dat", n_channels=3, irig_channel=2)
decoder = IRIGDecoder.from_video("behavior.avi", roi=(10, 20, 10, 20))
decoder = IRIGDecoder.from_intervals(onsets, offsets=offsets)
decoder = IRIGDecoder.from_events("session.txt", format="medpc")

# All produce a ClockTable via the same method
clock_table = decoder.decode()
```

The standalone `decode_*` functions are still available and do the same thing — `IRIGDecoder` simply provides a consistent interface.

## Synchronizing Multiple Streams (ClockTable as Synchronizer)

The ClockTable is the synchronizer: it bridges each device's local clock to UTC. When two recordings are each decoded against the same UTC reference, converting them to a shared `time_origin` puts them on a common time axis — even though the original devices had independent, drifting clocks.

```python
import numpy as np
import pynapple as nap
import neurokairos

# --- Decode each stream independently ---
ephys_ct = neurokairos.decode_dat_irig("ephys.dat", n_channels=64, irig_channel=63)
video_ct = neurokairos.decode_video_irig("behavior.avi", roi=(10, 20, 10, 20))

# --- Build pynapple time series using ClockTable as the synchronizer ---

# For ephys: convert sample indices to UTC, then to relative seconds
sample_indices = np.arange(0, 120000, dtype=np.float64)  # 4 seconds at 30 kHz
utc_times = ephys_ct.source_to_reference(sample_indices)
time_origin = ephys_ct.reference[0]                       # first UTC timestamp
ephys_tsd = nap.Tsd(t=utc_times - time_origin, d=my_neural_data)

# For video: convert frame indices to UTC, then to relative seconds
frame_indices = np.arange(len(x_positions), dtype=np.float64)
utc_times = video_ct.source_to_reference(frame_indices)
tracking_tsd = nap.Tsd(t=utc_times - time_origin, d=x_positions)
#                                       ^^^^^^^^^^^
#                        use the SAME time_origin for both streams

# --- Analyze together in pynapple ---
# Both Tsd objects now share a common time axis (seconds since ephys start)
# You can compute cross-correlations, restrict to intervals, etc.
```

The key insight: because both streams were decoded against the same UTC reference, converting them to the same `time_origin` puts them on a common time axis.

## Decoding from Event Logs

Some behavioral apparatus (e.g., MedPC, custom CSV loggers) record IRIG pulses as timestamped events rather than continuous waveforms. NeuroKairos can decode IRIG from these event logs and simultaneously convert behavioral events to UTC.

### MedPC Files

```python
from neurokairos import IRIGDecoder

# Decode IRIG pulses from a MedPC file
decoder = IRIGDecoder.from_events(
    "session.txt",
    format="medpc",
    pulse_high_code=27,    # event code for IRIG pulse HIGH
    pulse_low_code=28,     # event code for IRIG pulse LOW
)
clock_table = decoder.decode()

# Extract behavioral events converted to UTC
events_utc = decoder.get_behavioral_events_utc()

# Save behavioral events as CSV
decoder.save_behavioral_events_csv("session_events_utc.csv")
```

### CSV/TSV Files

```python
decoder = IRIGDecoder.from_events(
    "events.csv",
    format="csv",
    pulse_high_code="IRIG_HIGH",
    pulse_low_code="IRIG_LOW",
)
clock_table = decoder.decode()
```

### Standalone Event Functions

You can also use the lower-level functions directly:

```python
from neurokairos import parse_medpc_file, extract_irig_pulses, convert_events_to_utc
from neurokairos import decode_intervals_irig

# Parse the event log
events = parse_medpc_file("session.txt")

# Extract IRIG pulse onset/offset times
onsets, offsets = extract_irig_pulses(events, high_code=27, low_code=28)

# Decode to a ClockTable
clock_table = decode_intervals_irig(onsets, offsets=offsets)

# Convert non-IRIG events to UTC
events_utc = convert_events_to_utc(events, clock_table)
```

## Data Format Reference

### Interleaved .dat Files

For flat binary files with interleaved int16 channels (e.g., from Open Ephys, Intan).

```python
ct = neurokairos.decode_dat_irig(
    dat_path,       # path to the .dat file
    n_channels,     # total number of interleaved channels
    irig_channel,   # zero-based index of the IRIG channel
    save=True,      # save to <dat_path>.clocktable.npz (default True)
)
```

### SpikeGLX Files

For SpikeGLX `.bin` files. Reads `nSavedChans` and sample rate from the paired `.meta` file automatically.

```python
ct = neurokairos.decode_sglx_irig(
    bin_path,         # path to the .bin file (.meta must exist alongside)
    irig_channel,     # zero-based channel index, or "sync" for the last channel
    save=True,        # save to <bin_path>.clocktable.npz (default True)
)
```

The `"sync"` shortcut resolves to the last saved channel — this is where SpikeGLX stores the sync/digital word, which is the typical IRIG channel for nidq streams.

### Video Files

For video files with a visible IRIG LED. Requires `opencv-python`.

```python
ct = neurokairos.decode_video_irig(
    video_path,     # path to the video file (AVI, MP4, etc.)
    roi,            # (row_start, row_end, col_start, col_end) for the LED region
    fps=None,       # optional expected fps — warns if auto-estimate differs by >5%
    save=True,      # save to <video_path>.clocktable.npz (default True)
)
```

The `roi` defines the pixel region containing the IRIG LED. Pick a small rectangle tightly around the LED — a few pixels is enough. The decoder extracts mean grayscale brightness per frame from this region, then runs the standard IRIG pipeline on the resulting brightness signal.

To choose an ROI, open the video in any viewer, note the LED coordinates, and pass them as `(row_start, row_end, col_start, col_end)`. Rows are y (top to bottom), columns are x (left to right).

**Dropped frame diagnostics:**

```python
from neurokairos.video import dropped_frame_report

report = dropped_frame_report(ct, expected_fps=30.0)
print(f"Dropped {report['total_dropped']} of {report['total_expected']} frames")
print(f"Seconds with drops: {report['seconds_with_drops']}")
```

### Pre-extracted Pulse Intervals

For when you already have pulse onset/offset times (e.g., from Open Ephys event files, TTL loggers, or camera event CSVs).

```python
# From arrays of onset and offset times (in seconds)
ct = neurokairos.decode_intervals_irig(
    onsets,            # 1-D array of pulse onset times
    offsets=offsets,   # 1-D array of pulse offset times (required for arrays)
    source_units="seconds",  # label for the source axis (default "seconds")
    save="output.clocktable.npz",  # explicit path (no auto-save — no input file to derive from)
)

# From a pynapple IntervalSet (or any object with .start/.end attributes)
intervals = nap.IntervalSet(start=onsets, end=offsets)
ct = neurokairos.decode_intervals_irig(intervals, save="output.clocktable.npz")
```

## ClockTable Persistence

Every decoder saves its ClockTable alongside the input file by default:

| Decoder | Auto-save path |
|---|---|
| `decode_dat_irig` | `<dat_path>.clocktable.npz` |
| `decode_sglx_irig` | `<bin_path>.clocktable.npz` |
| `decode_video_irig` | `<video_path>.clocktable.npz` |
| `decode_intervals_irig` | No auto-save — pass `save="path.npz"` |

Reload with `ClockTable.load()`:

```python
ct = neurokairos.ClockTable.load("recording.dat.clocktable.npz")
```
